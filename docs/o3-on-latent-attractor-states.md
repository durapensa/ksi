Researchers are actively exploring *why* certain domains wield outsized influence on LLM reasoning. One hypothesis is that during pretraining, LLMs ingested vast amounts of text in certain domains (e.g. narrative fiction, Wikipedia articles, programming code), forming **latent attractor states**. When a prompt includes even a whiff of those domains, it pulls the model’s internal representation toward that subspace, biasing the generation. Support for this comes from analyses of model internals. *Zhu et al.* identify specialized **“contextual heads” in the transformer – attention heads that control global focus** – and show that distraction is linked to these heads focusing on the wrong tokens. Irrelevant domain text can misallocate attention, leading the model to overweight the distractor and under-attend the actual question \[**Zhu et al., arXiv 2025**\] . In essence, the model’s attention gets *attracted* to the domain cues (perhaps because similar tokens co-occurred frequently during training), acting like a gravitational pull in the token space. By tweaking key/query vectors for those heads (“focus directions”), Zhu et al. were able to re-align attention to relevant parts, mitigating the distraction without explicitly telling the model which tokens to ignore \[**Zhu et al., arXiv 2025**\] . This implies the **attractor phenomenon is real inside the network** – certain activation patterns corresponding to domain signals can hijack the model’s reasoning, and only by adjusting internal weights/attention can the effect be reduced.

Other evidence for attractor-like representations is the *“chain-of-thought trigger”* observed by Chatziveroglou et al. When a pathological instruction or irrelevant fact was added, models spontaneously produced a chain-of-thought, almost as if the added text implicitly activated a learned reasoning mode \[**Chatziveroglou et al., 2025**\] . The authors suggest this *implicit CoT prompting* happens because the model’s pretraining included many Q\&A examples where a tricky or complex input was followed by an explanation. Thus, the presence of certain phrases or structures in the prompt (even if meant as a distractor) **drops the model into an “explain mode” attractor**, where it starts reasoning verbosely. This unintended mode-switch supports the idea that LLMs have **latent attractor states tied to domains or styles** (e.g. a “step-by-step reasoning” state, a “conversational story” state, etc.) that can be triggered by contextual cues. Once triggered, the model’s outputs gravitate towards patterns typical of that mode, even if inappropriate for the task at hand.

