"""\nclaude_cli_litellm_provider.py - LiteLLM custom provider for Claude CLI\n\nA KSI-integrated provider that shells out to the claude CLI tool.\nDesigned specifically for KSI's needs, not as a general-purpose provider.\n\nKey features:\n- Progressive timeout strategy for long-running Claude operations\n- Progress monitoring to detect hung processes\n- Session continuity via --resume flag\n- Proper error mapping to LiteLLM/OpenAI conventions\n- Integration with KSI's configuration and logging\n"""\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport os\nimport subprocess\nimport time\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport litellm\nfrom litellm import CustomLLM\nfrom litellm.exceptions import (\n    Timeout,\n    APIError,\n    APIConnectionError,\n    BadRequestError,\n    ServiceUnavailableError\n)\n\n# Import KSI components\nfrom ksi_daemon.config import config\nfrom ksi_common.logging import get_logger\nfrom ksi_daemon.plugin_utils import get_logger\n\n# Configuration\nlogger = get_logger("claude_cli_provider")\nCLAUDE_BIN = Path(os.getenv("CLAUDE_BIN", "claude")).expanduser()\nDEFAULT_CLAUDE_MODEL = "sonnet"  # claude CLI only accepts "sonnet" or "opus"\n\n# Replicate model name detection constants\nREPLICATE_MODEL_NAME_WITH_ID_LENGTH = 64\n\n\nclass ClaudeCLIError(Exception):\n    """Custom error for Claude CLI provider issues"""\n    def __init__(self, status_code: int, message: str, stderr: str = ""):\n        self.status_code = status_code\n        self.message = message\n        self.stderr = stderr\n        super().__init__(self.message)\n\n\ndef build_cmd(\n    prompt: str,\n    *,\n    output_format: str = "json",\n    model_alias: str = DEFAULT_CLAUDE_MODEL,\n    allowed_tools: Optional[List[str]] = None,\n    disallowed_tools: Optional[List[str]] = None,\n    session_id: Optional[str] = None,\n    max_turns: Optional[int] = None,\n) -> List[str]:\n    """Compose the argv list for the Claude CLI."""\n    cmd = [\n        str(CLAUDE_BIN),\n        "-p",\n        "--output-format",\n        output_format,\n        "--model",\n        model_alias,\n    ]\n    if allowed_tools:\n        cmd += ["--allowedTools"] + allowed_tools\n    if disallowed_tools:\n        cmd += ["--disallowedTools"] + disallowed_tools\n    if max_turns is not None:\n        cmd += ["--max-turns", str(max_turns)]\n    if session_id:\n        cmd += ["--resume", session_id]\n    cmd.append(prompt)\n    return cmd\n\n\ndef allowed_tools_from_openai(tools_block: Optional[List[Dict[str, Any]]]) -> List[str]:\n    """Translate an OpenAI tools array into CLI allow-list names."""\n    if not tools_block:\n        return []\n    out: List[str] = []\n    for tool in tools_block:\n        if tool.get("type") == "function":\n            fn = tool.get("function", {})\n            if "name" in fn:\n                out.append(fn["name"])\n    return out\n\n\n\n\ndef map_subprocess_error_to_litellm(e: Exception, model: str) -> Exception:\n    """Map subprocess errors to appropriate LiteLLM/OpenAI-style exceptions"""\n    \n    if isinstance(e, subprocess.TimeoutExpired):\n        return Timeout(\n            message=f"Claude CLI timed out after {e.timeout}s",\n            model=model,\n            llm_provider="claude-cli"\n        )\n    \n    elif isinstance(e, subprocess.CalledProcessError):\n        # Map based on return code\n        if e.returncode in [-9, -15]:  # SIGKILL, SIGTERM\n            return ServiceUnavailableError(\n                message=f"Claude CLI terminated with signal {e.returncode}",\n                model=model,\n                llm_provider="claude-cli"\n            )\n        elif e.returncode == 1:  # General error - could be bad prompt\n            stderr = e.stderr if hasattr(e, 'stderr') else ""\n            return BadRequestError(\n                message=f"Claude CLI error: {stderr or 'Invalid request'}",\n                model=model,\n                llm_provider="claude-cli"\n            )\n        else:\n            return APIError(\n                message=f"Claude CLI failed with code {e.returncode}",\n                model=model,\n                llm_provider="claude-cli",\n                status_code=500\n            )\n    \n    elif isinstance(e, FileNotFoundError):\n        return APIConnectionError(\n            message=f"Claude CLI not found at {CLAUDE_BIN}",\n            model=model,\n            llm_provider="claude-cli"\n        )\n    \n    elif isinstance(e, ClaudeCLIError):\n        # Map our custom error based on status code\n        if e.status_code == 400:\n            return BadRequestError(\n                message=e.message,\n                model=model,\n                llm_provider="claude-cli"\n            )\n        else:\n            return APIError(\n                message=e.message,\n                model=model,\n                llm_provider="claude-cli",\n                status_code=e.status_code\n            )\n    \n    # Default to generic API error\n    return APIError(\n        message=f"Claude CLI error: {str(e)}",\n        model=model,\n        llm_provider="claude-cli",\n        status_code=500\n    )\n\n\nclass ClaudeCLIProvider(CustomLLM):\n    """\n    KSI-integrated LiteLLM provider for Claude CLI.\n    \n    Use via model="claude-cli/sonnet" or model="claude-cli/opus"\n    \n    This provider is designed specifically for KSI and integrates with:\n    - KSI's configuration system for timeouts\n    - KSI's structured logging\n    - KSI's error handling patterns\n    \n    Note: Streaming is not supported as Claude CLI doesn't provide it\n    and KSI doesn't use streaming completions.\n    """\n\n    _llm_provider = "claude-cli"\n    \n    def __init__(self):\n        super().__init__()\n        # Dedicated executor for long-running Claude operations\n        self.claude_executor = ThreadPoolExecutor(\n            max_workers=config.claude_max_workers,\n            thread_name_prefix="claude-cli"\n        )\n        logger.info(\n            "Claude CLI provider initialized",\n            claude_bin=str(CLAUDE_BIN),\n            max_workers=config.claude_max_workers,\n            timeout_attempts=config.claude_timeout_attempts,\n            progress_timeout=config.claude_progress_timeout\n        )\n\n    # ------------------------- public sync entry-points ---------------------- #\n\n    def completion(self, messages, *args, **kwargs):\n        """Synchronous completion - not supported in KSI daemon context"""\n        if kwargs.get("stream"):\n            raise NotImplementedError(\n                "Claude CLI provider does not support streaming. "\n                "KSI uses non-streaming completions only."\n            )\n        \n        # Check if we're in an async context\n        try:\n            loop = asyncio.get_running_loop()\n            # We're in an event loop - this shouldn't happen in KSI\n            raise RuntimeError(\n                "Synchronous completion() called from async context. "\n                "KSI daemon should use acompletion() instead. "\n                "Check that completion_service.py and litellm.py are using await litellm.acompletion()"\n            )\n        except RuntimeError as e:\n            if "no running event loop" in str(e).lower():\n                # No event loop running, we can use asyncio.run() for testing\n                return asyncio.run(self._acompletion(messages, *args, **kwargs))\n            else:\n                # Re-raise other RuntimeErrors\n                raise\n\n    def streaming(self, messages, *args, **kwargs):\n        """Streaming not supported - Claude CLI doesn't provide incremental output"""\n        raise NotImplementedError(\n            "Claude CLI provider does not support streaming. "\n            "Use stream=False for non-streaming completions."\n        )\n\n    # ------------------------- public async entry-points --------------------- #\n\n    async def acompletion(self, messages, *args, **kwargs):\n        """Async completion - main entry point for KSI"""\n        if kwargs.get("stream"):\n            raise NotImplementedError(\n                "Claude CLI provider does not support streaming. "\n                "KSI uses non-streaming completions only."\n            )\n        return await self._acompletion(messages, *args, **kwargs)\n\n    async def astreaming(self, messages, *args, **kwargs):\n        """Async streaming not supported"""\n        raise NotImplementedError(\n            "Claude CLI provider does not support streaming. "\n            "Use stream=False for non-streaming completions."\n        )\n\n    # ------------------------- internal implementation ----------------------- #\n\n    async def _acompletion(self, messages, *args, **kwargs):\n        """Claude CLI execution with intelligent retry logic"""\n        prompt, model_alias = self._extract_prompt_and_model(messages, *args, **kwargs)\n        full_model = f"claude-cli/{model_alias}"\n        \n        # Get timeouts from KSI config\n        timeouts = config.claude_timeout_attempts  # [300, 900, 1800] = 5min, 15min, 30min\n        \n        logger.info(\n            "Starting Claude CLI completion",\n            model=model_alias,\n            session_id=kwargs.get("session_id"),\n            timeout_strategy=timeouts\n        )\n        \n        for attempt, timeout in enumerate(timeouts):\n            allowed = allowed_tools_from_openai(kwargs.get("tools"))\n            disallowed = kwargs.get("disallowed_tools") or []\n            session_id = kwargs.get("session_id")\n            max_turns = kwargs.get("max_turns")\n\n            cmd = build_cmd(\n                prompt,\n                output_format="json",\n                model_alias=model_alias,\n                allowed_tools=allowed,\n                disallowed_tools=disallowed,\n                session_id=session_id,\n                max_turns=max_turns,\n            )\n            \n            try:\n                logger.debug(\n                    f"Attempt {attempt + 1}/{len(timeouts)}",\n                    timeout=timeout,\n                    session_id=session_id\n                )\n                \n                # Use thread executor for long-running Claude operations\n                loop = asyncio.get_event_loop()\n                result = await loop.run_in_executor(\n                    self.claude_executor,\n                    self._run_claude_sync_with_progress,\n                    cmd,\n                    timeout,\n                    full_model\n                )\n                \n                # Success - process the result\n                return self._process_claude_result(result, model_alias, prompt)\n                \n            except subprocess.TimeoutExpired as e:\n                if attempt < len(timeouts) - 1:  # Not final attempt\n                    logger.warning(\n                        f"Claude timeout after {timeout}s, attempt {attempt + 1}/{len(timeouts)}",\n                        next_timeout=timeouts[attempt + 1]\n                    )\n                    # Fresh session on timeout (process may have been hanging)\n                    kwargs.pop("session_id", None)\n                    await asyncio.sleep(config.claude_retry_backoff)\n                else:\n                    # Final attempt failed - map to LiteLLM exception\n                    raise map_subprocess_error_to_litellm(e, full_model)\n                    \n            except Exception as e:\n                # Map all errors to LiteLLM exceptions\n                raise map_subprocess_error_to_litellm(e, full_model)\n    \n    def _run_claude_sync_with_progress(self, cmd: List[str], timeout: int, model: str):\n        """Run Claude with cross-platform progress monitoring"""\n        progress_timeout = config.claude_progress_timeout  # 5 minutes default\n        \n        logger.debug(\n            "Executing Claude CLI",\n            cmd=" ".join(cmd),\n            timeout=timeout,\n            progress_timeout=progress_timeout\n        )\n        \n        # Set working directory to project root (matching daemon behavior)\n        project_root = Path(__file__).parent.parent.parent.parent\n        \n        start_time = time.time()\n        last_output_time = time.time()\n        stdout_chunks = []\n        stderr_chunks = []\n        output_lock = threading.Lock()\n        \n        def update_last_output_time():\n            nonlocal last_output_time\n            with output_lock:\n                last_output_time = time.time()\n        \n        def read_stream(stream, chunks, stream_name):\n            """Read from stream in a separate thread"""\n            try:\n                while True:\n                    chunk = stream.read(1024)\n                    if not chunk:\n                        break\n                    chunks.append(chunk)\n                    update_last_output_time()\n                    if stream_name == "stderr" and chunk.strip():\n                        logger.debug(f"Claude stderr: {chunk.strip()}")\n            except ValueError:\n                # Stream closed\n                pass\n            except Exception as e:\n                logger.error(f"Error reading {stream_name}: {e}")\n        \n        try:\n            # Start process\n            process = subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                cwd=str(project_root),\n                env=os.environ\n            )\n            \n            # Start reader threads for cross-platform compatibility\n            stdout_thread = threading.Thread(\n                target=read_stream, \n                args=(process.stdout, stdout_chunks, "stdout"),\n                daemon=True\n            )\n            stderr_thread = threading.Thread(\n                target=read_stream, \n                args=(process.stderr, stderr_chunks, "stderr"),\n                daemon=True\n            )\n            \n            stdout_thread.start()\n            stderr_thread.start()\n            \n            # Monitor progress and timeouts\n            while process.poll() is None:\n                current_time = time.time()\n                elapsed = current_time - start_time\n                \n                # Check if no output for progress_timeout seconds (might be hanging)\n                with output_lock:\n                    time_since_output = current_time - last_output_time\n                    if time_since_output > progress_timeout:\n                        logger.error(\n                            f"No output for {progress_timeout}s, killing process",\n                            elapsed=elapsed\n                        )\n                        process.kill()\n                        process.wait()\n                        raise subprocess.TimeoutExpired(cmd, progress_timeout)\n                \n                # Check overall timeout\n                if elapsed > timeout:\n                    logger.error(\n                        f"Overall timeout {timeout}s exceeded, killing process",\n                        elapsed=elapsed\n                    )\n                    process.kill()\n                    process.wait()\n                    raise subprocess.TimeoutExpired(cmd, timeout)\n                \n                # Sleep briefly to avoid busy waiting\n                time.sleep(1)\n            \n            # Wait for reader threads to finish\n            stdout_thread.join(timeout=5)\n            stderr_thread.join(timeout=5)\n            \n            # Combine all output\n            stdout = ''.join(stdout_chunks)\n            stderr = ''.join(stderr_chunks)\n            \n            # Check return code\n            if process.returncode != 0:\n                logger.error(\n                    f"Claude CLI failed with code {process.returncode}",\n                    stderr=stderr[:500]  # First 500 chars of stderr\n                )\n                raise subprocess.CalledProcessError(process.returncode, cmd, stdout, stderr)\n            \n            # Return object that mimics subprocess.CompletedProcess\n            class CompletedProcessResult:\n                def __init__(self, returncode, stdout, stderr):\n                    self.returncode = returncode\n                    self.stdout = stdout\n                    self.stderr = stderr\n                    self.args = cmd\n            \n            elapsed = time.time() - start_time\n            logger.info(\n                f"Claude CLI completed successfully",\n                elapsed=f"{elapsed:.1f}s",\n                output_size=len(stdout)\n            )\n            \n            return CompletedProcessResult(process.returncode, stdout, stderr)\n            \n        except Exception as e:\n            # Ensure process is terminated on any error\n            if 'process' in locals():\n                try:\n                    process.kill()\n                    process.wait()\n                except:\n                    pass\n            raise\n    \n    def _process_claude_result(self, result, model_alias: str, prompt: str):\n        """Process successful Claude CLI result and create LiteLLM response"""\n        raw_response = result.stdout\n        stderr_output = result.stderr\n        \n        # For KSI, we pass through the raw JSON response\n        # The completion_service.py will parse and wrap it properly\n        # KSI always uses --output-format json, so we expect valid JSON\n        \n        # Create LiteLLM response with raw JSON\n        response = litellm.completion(  # type: ignore\n            model=f"claude-cli/{model_alias}",\n            mock_response=raw_response,  # Pass raw JSON string\n            messages=[{"role": "user", "content": prompt}],\n        )\n        \n        # Attach metadata for debugging and compatibility\n        response._raw_stdout = raw_response\n        response._stderr = stderr_output\n        \n        # Try to extract sessionId for convenience\n        try:\n            claude_data = json.loads(raw_response)\n            if "sessionId" in claude_data:\n                response.sessionId = claude_data["sessionId"]\n            response._claude_metadata = claude_data\n            \n            logger.debug(\n                "Created LiteLLM response",\n                has_session_id="sessionId" in claude_data,\n                response_type=claude_data.get("type"),\n                is_error=claude_data.get("is_error", False)\n            )\n        except json.JSONDecodeError as e:\n            logger.warning(\n                f"Claude CLI returned invalid JSON: {e}",\n                raw_response_preview=raw_response[:200]\n            )\n            response._json_decode_error = str(e)\n            \n        return response\n\n    def _extract_prompt_and_model(self, messages, *args, **kwargs):\n        """Extract prompt and validate model from LiteLLM kwargs"""\n        prompt = messages[-1]["content"]\n        \n        # Extract model from kwargs (LiteLLM passes it here)\n        full_model = kwargs.get("model", f"claude-cli/{DEFAULT_CLAUDE_MODEL}")\n        \n        # Extract the part after provider prefix using LiteLLM pattern\n        # This handles both "claude-cli/sonnet" and edge cases like just "sonnet"\n        parts = full_model.split("/", 1)\n        if len(parts) == 2 and parts[0] == "claude-cli":\n            # Standard format: claude-cli/model\n            model_alias = parts[1]\n        elif len(parts) == 1:\n            # Just model name without provider prefix\n            model_alias = parts[0]\n        else:\n            # Unexpected format, use default\n            model_alias = DEFAULT_CLAUDE_MODEL\n            \n        # Validate model - claude CLI only accepts "sonnet" or "opus"\n        if model_alias not in ["sonnet", "opus"]:\n            logger.warning(\n                f"Claude CLI model '{model_alias}' not supported, using {DEFAULT_CLAUDE_MODEL}",\n                requested_model=model_alias,\n                valid_models=["sonnet", "opus"]\n            )\n            model_alias = DEFAULT_CLAUDE_MODEL\n            \n        return prompt, model_alias\n\n\n# Register provider with LiteLLM\n_provider = ClaudeCLIProvider()\nlitellm.custom_provider_map.append(\n    {"provider": "claude-cli", "custom_handler": _provider}\n)\n\nlogger.info("Claude CLI provider registered with LiteLLM")\n\n\n# Quick self-test when run directly\nif __name__ == "__main__":\n    import sys\n    \n    # Configure basic logging for testing\n    import logging\n    logging.basicConfig(level=logging.DEBUG)\n    \n    user_prompt = sys.argv[1] if len(sys.argv) > 1 else "Hello! Please respond with a simple greeting."\n    \n    print("Testing Claude CLI provider...")\n    print(f"Prompt: {user_prompt}")\n    print("-" * 50)\n    \n    try:\n        resp = litellm.completion(\n            model="claude-cli/sonnet",\n            messages=[{"role": "user", "content": user_prompt}],\n            max_turns=1,\n        )\n        \n        print("Response:")\n        print(resp.choices[0].message.content)\n        \n        if hasattr(resp, 'sessionId'):\n            print(f"\nSession ID: {resp.sessionId}")\n            \n    except Exception as e:\n        print(f"Error: {type(e).__name__}: {e}")\n        sys.exit(1)