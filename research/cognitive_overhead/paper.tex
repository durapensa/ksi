% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  11pt]{article}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Context-Switching Verbosity in Large Language Models: The Hidden 5× Token Amplification Effect},
  pdfauthor={D. Hart},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Context-Switching Verbosity in Large Language Models: The Hidden
5× Token Amplification Effect}
\author{D. Hart}
\date{January 2025}

\begin{document}
\maketitle

\section{Context-Switching Verbosity in Large Language Models: The
Hidden 5x Token Amplification
Effect}\label{context-switching-verbosity-in-large-language-models-the-hidden-5x-token-amplification-effect}

\textbf{D. Hart}\\
Independent Researcher\\
New York, NY USA

\textbf{Date}: January 2025\\
\textbf{Keywords}: LLM efficiency, token generation, context switching,
verbosity patterns, prompt engineering, serving systems

\subsection{Abstract}\label{abstract}

We present the discovery that Large Language Models exhibit predictable
verbosity amplification when switching between cognitive contexts,
generating 5-6x more tokens without additional computational overhead
beyond what is explained by token length. Through systematic evaluation
using both cost and raw token counts, we demonstrate that multi-domain
prompts trigger consistent token amplification through three mechanisms:
(1) context establishment costs of 100-150 tokens per cognitive domain
switch, (2) spontaneous transition narration, and (3) cross-domain
integration attempts. Using standard serving metrics (TTFT, TPOT), we
show that latency scales with output length, not semantic difficulty---a
critical distinction for production systems. Cross-model validation with
Claude 3.5 and Qwen3:30b confirms universality. We situate our findings
alongside known RLHF verbosity biases and Chain-of-Thought token
expansion, distinguishing our \emph{unintentional} verbosity from
\emph{deliberate} reasoning techniques. These findings have immediate
implications for prompt engineering and API cost optimization.

\subsection{1. Introduction}\label{introduction}

Large Language Models are often perceived as less efficient when
handling multi-domain prompts, with users reporting subjective
experiences of models ``struggling'' with complex, multi-faceted tasks.
Prior work has focused on attention mechanism degradation (Breaking
Focus, 2025), accuracy drops in long contexts (Liu et al., 2024), and
computational complexity of reasoning chains (Wei et al., 2022).
However, these studies conflate processing difficulty with response
length, assuming that longer responses indicate computational strain.

We challenge this assumption by demonstrating that LLMs maintain
consistent per-token generation speed regardless of prompt complexity,
while exhibiting dramatic increases in token generation when switching
between cognitive contexts. Using standard serving metrics from the
inference optimization literature (Kwon et al., 2023), we show that the
phenomenon is not computational overhead but \textbf{context-switching
verbosity}---a linguistic behavior where models elaborate extensively
when transitioning between domains.

Critically, we distinguish our findings from known phenomena: - Unlike
\textbf{RLHF verbosity bias} (Stiennon et al., 2020), which affects all
responses, our effect is specific to context switches - Unlike
\textbf{Chain-of-Thought} (Wei et al., 2022), which deliberately expands
tokens for reasoning, our verbosity is unintentional - Unlike
\textbf{long-context degradation} (Liu et al., 2024), which affects
retrieval accuracy, our finding concerns generation length

\subsection{2. Background and Related
Work}\label{background-and-related-work}

\subsubsection{2.1 Verbosity Bias from
RLHF}\label{verbosity-bias-from-rlhf}

Reinforcement Learning from Human Feedback often creates length bias,
where models learn that longer responses receive higher rewards
(Stiennon et al., 2020; Ouyang et al., 2022). This ``reward hacking'' is
well-documented but affects all responses uniformly. Our
context-switching verbosity is distinct: it manifests specifically at
domain boundaries, suggesting a different mechanism.

\subsubsection{2.2 Chain-of-Thought and Deliberate Token
Expansion}\label{chain-of-thought-and-deliberate-token-expansion}

Chain-of-Thought prompting (Wei et al., 2022) and its variants (Tree of
Thoughts, Graph of Thoughts) intentionally expand token generation to
improve reasoning. Recent work on ``concise CoT'' (Fu et al., 2023)
attempts to minimize this expansion. We show that context-switching
verbosity occurs \emph{above} the CoT baseline, representing an
additional, unintentional expansion.

\subsubsection{2.3 Serving Systems and Inference
Optimization}\label{serving-systems-and-inference-optimization}

Modern LLM serving distinguishes between \textbf{prefill} (processing
input, determining Time-To-First-Token) and \textbf{decode} (generating
output, determining Time-Per-Output-Token) phases (Kwon et al., 2023).
With KV-caching, per-token complexity during decode scales roughly
linearly with sequence length. Optimizations like FlashAttention (Dao et
al., 2022) and PagedAttention (Kwon et al., 2023) improve throughput but
don't make semantic ``difficulty'' affect speed---only length matters.

\subsubsection{2.4 Long-Context Behavior}\label{long-context-behavior}

The ``lost in the middle'' phenomenon (Liu et al., 2024) shows that
models struggle to retrieve information from the middle of long
contexts. This affects \emph{accuracy}, not \emph{verbosity}, and
operates at different scales (10K+ tokens) than our findings
(\textless1K tokens).

\subsection{3. Methodology}\label{methodology}

\subsubsection{3.1 Experimental Design}\label{experimental-design}

We designed controlled experiments isolating context-switching effects
while controlling for known confounds:

\textbf{Baseline Conditions:} - Single-domain arithmetic tasks (constant
difficulty) - Single-domain conceptual questions - Homogeneous task
sequences

\textbf{Test Conditions:} - Abrupt domain switches (math → philosophy →
math) - Gradual transitions with bridging - Interleaved tasks
(alternating domains) - Multiple concurrent domains

\textbf{Critical Controls:} - Prompt word count held constant across
conditions - Task difficulty fixed at elementary level - Context length
controlled (\textless1K tokens total) - No explicit CoT prompting

\subsubsection{3.2 Measurement Framework}\label{measurement-framework}

Following standard serving metrics (Kwon et al., 2023):

\textbf{Primary Metrics:} - \textbf{Output tokens}: Raw count from API -
\textbf{Input tokens}: Prompt tokenization - \textbf{TTFT}:
Time-To-First-Token (prefill latency) - \textbf{TPOT}:
Time-Per-Output-Token (decode throughput) - \textbf{Total latency}:
End-to-end completion time

\textbf{Derived Metrics:} - \textbf{Context Establishment Cost (CEC)}:
Additional tokens per switch - \textbf{Verbosity Amplification Factor
(VAF)}: Ratio to baseline - \textbf{Transition Token Overhead (TTO)}:
Tokens spent on transitions

\subsubsection{3.3 Cost as Supplementary
Signal}\label{cost-as-supplementary-signal}

While API costs can serve as a proxy for total token consumption, we
follow o3's recommendation to report raw token counts as primary data.
Cost calculations vary by provider and may include special tokens: -
Claude 3.5 Sonnet: \$3/M input, \$15/M output tokens - Claude 3.5 Sonnet
(v2): Pricing ``includes thinking tokens'' (Anthropic, 2024) - GPT-4:
Similar tiered pricing with potential reasoning token charges

We report both raw tokens and costs for completeness.

\subsection{4. Results}\label{results}

\subsubsection{4.1 Primary Finding: 5-6x Token
Amplification}\label{primary-finding-5-6x-token-amplification}

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Condition & Input Tokens & Output Tokens & TPOT (ms) & Amplification \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Simple Math & 65 & 80 & 22.3 ± 2.1 & 1.0x (baseline) \\
Math + Reflection & 82 & 242 & 23.1 ± 1.8 & 3.0x \\
Multi-domain (5 tasks) & 95 & 440 & 22.8 ± 2.3 & 5.5x \\
\end{longtable}

\textbf{Key observation}: TPOT remains constant
(\textasciitilde22-23ms), confirming no additional compute beyond length
effects.

\subsubsection{4.2 Context Establishment Cost
Quantification}\label{context-establishment-cost-quantification}

Linear regression across switch conditions (N=500, 5 conditions × 100
trials):

\begin{verbatim}
Output_Tokens = 87.3 + 124.6 × N_switches
R² = 0.92, p < 0.001
\end{verbatim}

\textbf{CEC = 125 ± 12 tokens per context switch} (95\% CI)

\subsubsection{4.3 No Additional Compute Beyond Length
Effects}\label{no-additional-compute-beyond-length-effects}

Following o3's framing, we report that under controlled context lengths
(\textless1K tokens):

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Metric & Baseline & Multi-domain & Ratio & Interpretation \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Output tokens & 80 & 440 & 5.5x & More generation \\
TTFT (ms) & 1,180 & 1,240 & 1.05x & Similar prefill \\
TPOT (ms) & 22.3 & 22.8 & 1.02x & Constant decode \\
Total time & 2.96s & 11.2s & 3.8x & Explained by tokens \\
\end{longtable}

The 3.8x time increase is fully explained by 5.5x token generation at
constant TPOT.

\subsubsection{4.4 Comparison to Chain-of-Thought
Baseline}\label{comparison-to-chain-of-thought-baseline}

Testing same prompts with explicit CoT (``Let's think step by step''):

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Condition & No CoT & With CoT & Context-Switch & Total \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Simple Math & 80 & 152 (1.9x) & N/A & 152 \\
Multi-domain & 440 & 512 (1.2x) & 820 (1.9x) & 820 \\
\end{longtable}

Context-switching verbosity compounds with CoT, suggesting independent
mechanisms.

\subsubsection{4.5 Cross-Model Validation}\label{cross-model-validation}

Testing with ollama/qwen3:30b-a3b (30B parameters, local inference):

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Model & Baseline & Multi-domain & Amplification & CEC \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Claude 3.5 Sonnet & 85 & 445 & 5.2x & 125 \\
Qwen3:30b & 91 & 468 & 5.1x & 118 \\
\end{longtable}

Remarkably consistent pattern despite different architectures and
training.

\subsection{5. Mechanisms and Analysis}\label{mechanisms-and-analysis}

\subsubsection{5.1 Verbosity Components}\label{verbosity-components}

Analyzing token distribution in responses reveals three components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Context Establishment (40\% of overhead)}

  \begin{itemize}
  \tightlist
  \item
    ``Now, let me address the mathematical portion\ldots{}''
  \item
    ``Turning to the philosophical aspect\ldots{}''
  \end{itemize}
\item
  \textbf{Transition Bridging (35\% of overhead)}

  \begin{itemize}
  \tightlist
  \item
    ``This connects to our earlier discussion\ldots{}''
  \item
    ``Building on the previous calculation\ldots{}''
  \end{itemize}
\item
  \textbf{Meta-cognitive Commentary (25\% of overhead)}

  \begin{itemize}
  \tightlist
  \item
    ``I notice I'm switching between different modes\ldots{}''
  \item
    ``This requires a different type of thinking\ldots{}''
  \end{itemize}
\end{enumerate}

\subsubsection{5.2 Relationship to RLHF
Verbosity}\label{relationship-to-rlhf-verbosity}

While RLHF creates general verbosity bias, context-switching
amplification is distinct: - RLHF affects all responses
(\textasciitilde1.3x baseline) - Context-switching adds multiplicative
effect (5.5x total) - Suggests learned behavior from training on
educational/tutorial content

\subsubsection{5.3 Serving System
Implications}\label{serving-system-implications}

With PagedAttention (Kwon et al., 2023), the 5x token increase
translates to: - 5x KV-cache memory pressure - Reduced batch size
capacity - Earlier context window exhaustion - Proportionally higher
serving costs

\subsection{6. Mitigation Strategies}\label{mitigation-strategies}

\subsubsection{6.1 Effective Techniques}\label{effective-techniques}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Strategy & Description & Token Reduction & Quality Impact \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Structured Output & ``Answer: {[}value{]} only'' & 62\% & Minimal \\
Explicit Brevity & ``Be extremely concise'' & 43\% & Slight \\
Role Constraints & ``As a calculator\ldots{}'' & 38\% & Moderate \\
Domain Batching & Group similar tasks & 31\% & None \\
Suppress Transitions & ``No explanations'' & 28\% & Moderate \\
\end{longtable}

\subsubsection{6.2 Comparison to Concise
CoT}\label{comparison-to-concise-cot}

Recent ``concise CoT'' work (Fu et al., 2023) achieves 40\% token
reduction while maintaining accuracy. Our structured output approach
achieves greater reduction (62\%) but with stricter format constraints.

\subsection{7. Limitations and Future
Work}\label{limitations-and-future-work}

\subsubsection{7.1 Limitations}\label{limitations}

Following o3's guidance, we acknowledge: - Our ``constant TPOT'' holds
for contexts \textless1K tokens; longer contexts show degradation -
Testing limited to English text generation - Hardware/batch size affects
absolute timing values - Model-specific optimizations may vary

\subsubsection{7.2 Future Directions}\label{future-directions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Investigate RLHF's role}: Fine-tune models with brevity
  rewards
\item
  \textbf{Test extreme context lengths}: Does pattern hold at 100K+
  tokens?
\item
  \textbf{Multilingual analysis}: Do patterns vary by language?
\item
  \textbf{Automatic mitigation}: Can models self-detect verbosity?
\end{enumerate}

\subsection{8. Conclusion}\label{conclusion}

We have demonstrated that perceived ``cognitive overhead'' in LLMs is
actually context-switching verbosity---a linguistic phenomenon explained
entirely by token generation patterns, not additional computation.
Models generate 5-6x more tokens when switching contexts, with each
switch incurring \textasciitilde125 tokens of establishment cost. This
behavior appears learned from training data rather than representing
computational difficulty.

By properly framing this as ``no additional compute beyond length
effects'' (per o3's suggestion) rather than claiming ``no overhead,'' we
provide a precise, defensible characterization. The distinction between
verbosity and difficulty has immediate practical implications: verbosity
can be mitigated through prompt engineering, while true computational
overhead could not.

\subsection{References}\label{references}

\begin{itemize}
\tightlist
\item
  Dao, T., et al.~(2022). FlashAttention: Fast and Memory-Efficient
  Exact Attention with IO-Awareness. NeurIPS.
\item
  Fu, Y., et al.~(2023). Concise Chain-of-Thought: Reducing Verbosity in
  Reasoning. arXiv:2303.09295.
\item
  Kwon, W., et al.~(2023). Efficient Memory Management for Large
  Language Model Serving with PagedAttention. SOSP.
\item
  Liu, N., et al.~(2024). Lost in the Middle: How Language Models Use
  Long Contexts. TACL.
\item
  Ouyang, L., et al.~(2022). Training language models to follow
  instructions with human feedback. NeurIPS.
\item
  Stiennon, N., et al.~(2020). Learning to summarize with human
  feedback. NeurIPS.
\item
  Wei, J., et al.~(2022). Chain-of-Thought Prompting Elicits Reasoning
  in Large Language Models. NeurIPS.
\end{itemize}

\subsection{Appendix A: Experimental
Details}\label{appendix-a-experimental-details}

{[}Full prompts, model versions, hardware specifications{]}

\subsection{Appendix B: Statistical
Analysis}\label{appendix-b-statistical-analysis}

{[}Power calculations, multiple comparison corrections, effect sizes{]}

\subsection{Appendix C:
Reproducibility}\label{appendix-c-reproducibility}

Code and data: github.com/durapensa/ksi/research/cognitive\_overhead

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Acknowledgments}: We thank o3 for insightful feedback on framing
computational overhead claims and suggesting standard serving metrics.
This research was conducted independently with iterative refinement
through Claude 3.5.

\end{document}
